{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2nenZ1iJJtSMO2gYIZDI+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alimumtaz95/Q2_assignments/blob/main/Project_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Project 01: Langchain & Google Gemini\n",
        "Installing the required genai and Langchain SDK"
      ],
      "metadata": {
        "id": "2HJBSYV-uQbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U langchain-google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shRSiS9fuRPX",
        "outputId": "8267ca5e-523d-4dd4-dc3e-eaffe5c6e169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "GEMINI_API_KEY: str = userdata.get('Gemini_API_Key')\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-exp\")"
      ],
      "metadata": {
        "id": "ywmaYGcou59q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Simple Question Answering\n",
        "prompt_template_1 = \"\"\"You are a helpful assistant. Answer the following question: {question} do not add more details just give one line answer\"\"\"\n",
        "prompt_1 = ChatPromptTemplate.from_template(prompt_template_1)\n",
        "chain = prompt_1 | llm | StrOutputParser()\n",
        "\n",
        "# 3. Run Chains with User-Defined Questions\n",
        "\n",
        "# Questions for Chain 1 (Question Answering)\n",
        "questions_1 = [\n",
        "    \"What is the capital of France?\",\n",
        "    \"What are the benefits of using LangChain?\",\n",
        "    \"What is the meaning of life?\"\n",
        "]\n",
        "\n",
        "print(\"--- Chain 1: Question Answering ---\")\n",
        "for question in questions_1:\n",
        "    response = chain.invoke({\"question\": question})\n",
        "    display(Markdown(f\"# **Question:** \\n {question}\"))\n",
        "    display(Markdown(f\"## **Answer:** \\n {response}\\n\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "-iPuQfhKvhJ-",
        "outputId": "10504b51-549a-4a8c-d103-e4c791491acf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Chain 1: Question Answering ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# **Question:** \n What is the capital of France?"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## **Answer:** \n The capital of France is Paris.\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# **Question:** \n What are the benefits of using LangChain?"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## **Answer:** \n LangChain provides a framework for building applications using large language models.\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# **Question:** \n What is the meaning of life?"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## **Answer:** \n The meaning of life is subjective and varies from person to person.\n\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}